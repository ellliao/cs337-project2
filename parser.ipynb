{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import recipe as r\n",
    "\n",
    "def find_ingredient(name: str, ingredients: list[r.Ingredient]) -> list[int]:\n",
    "    '''Finds the list of indices of the ingredients possibly being referenced'''\n",
    "\n",
    "    ingr_inds = []\n",
    "    components = name.split()\n",
    "\n",
    "    # Remove any determiners\n",
    "    if components[0] in ['the', 'a', 'an']:\n",
    "        del components[0]\n",
    "        name = ' '.join(components)\n",
    "\n",
    "    # If name is empty, return\n",
    "    if not name:\n",
    "        return ingr_inds\n",
    "    \n",
    "    # Initialize confidence to 1/2 of number of words\n",
    "    max_confidence = len(components) / 2\n",
    "\n",
    "    for i in range(len(ingredients)):\n",
    "        if name == ingredients[i].name:\n",
    "            # Return if an exact match\n",
    "            return [i]\n",
    "        elif name in ingredients[i].name:\n",
    "            # If a substring, add to list and limit search to other substrs\n",
    "            ingr_inds.append(i)\n",
    "            max_confidence = len(components) + 1\n",
    "        elif max_confidence <= len(components):\n",
    "            # Check how many words match and add to list if == current max or\n",
    "            # reset list to just it + update max if >\n",
    "            confidence = 0\n",
    "            for wd in components:\n",
    "                if wd in ingredients[i].name:\n",
    "                    confidence += 1\n",
    "            if confidence > max_confidence:\n",
    "                ingr_inds = [i]\n",
    "            elif confidence == max_confidence:\n",
    "                ingr_inds.append(i)\n",
    "    \n",
    "    return ingr_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import util as u\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def parse_and_add_step(instr: str, recipe: r.Recipe) -> None:\n",
    "    '''Parses an instruction into maybe more steps and adds to recipe.'''\n",
    "\n",
    "    # Split recipe instruction into \"sentences\" for SpaCy parser\n",
    "    steps = re.split(r'(?:\\.|;|, then)\\s+', instr)\n",
    "\n",
    "    for text in steps:\n",
    "\n",
    "        # Skip if empty\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        # Form a nice sentence for the step text, and initialize step\n",
    "        text = ''.join([text[0].upper(), text[1:len(text)]])\n",
    "        if not text.endswith('.'):\n",
    "            text = ''.join([text, '.'])\n",
    "        if len(recipe.steps) > 0:\n",
    "            step: r.Step = r.Step(text, recipe.steps[-1].state)\n",
    "        else:\n",
    "            step: r.Step = r.Step(text, r.IngredientState(recipe.ingredients))\n",
    "\n",
    "        # Uncapitalize the first letter. This prevents SpaCy from reading the\n",
    "        # first word as a proper noun (imperative sentences are less common in\n",
    "        # its dataset).\n",
    "        text = ''.join([text[0].lower(), text[1:len(text)]])\n",
    "\n",
    "        # Based on the dependency + part of speech tagging, extract necessary\n",
    "        # information.\n",
    "        doc = nlp(text)\n",
    "        for (i, token) in enumerate(doc):\n",
    "\n",
    "            # If the parser interpreted an imperative sentence as an NP,\n",
    "            # correct it.\n",
    "            if token.dep_ == 'ROOT' and token.head.pos_ == 'NOUN':\n",
    "                for j in range(i,-1,-1):\n",
    "                    if j == 0 or \\\n",
    "                        (j > 0 and doc[j-1].dep_ == 'punct'):\n",
    "                        doc[j].dep_ = 'ROOT'\n",
    "                        token.dep_ = 'dobj'\n",
    "                        step.methods.append(doc[j].text)\n",
    "                        break\n",
    "\n",
    "            # Extract methods\n",
    "            if token.dep_ == 'ROOT':\n",
    "                step.methods.append(token.text)\n",
    "            elif token.dep_ == 'conj':\n",
    "                if token.head.text in step.methods:\n",
    "                    step.methods.append(token.text)\n",
    "\n",
    "        # Extract ingredients and tools\n",
    "        for chunk in doc.noun_chunks:\n",
    "\n",
    "            # If the root verb is mistakenly in the chunk, remove it.\n",
    "            name = chunk.text\n",
    "            if chunk.root.dep_ == 'ROOT' and step.methods[0] in name:\n",
    "                name = name.partition(chunk.root.text)[2]\n",
    "                if not name:\n",
    "                    continue\n",
    "                name = name[1:-1]\n",
    "            \n",
    "            # If the noun chunk is not a dobj, pobj, or conj, ignore it.\n",
    "            if chunk.root.dep_ not in ['dobj', 'pobj', 'conj']:\n",
    "                continue\n",
    "            \n",
    "            # Check the type of noun\n",
    "            ntype = u.NounType.from_str(chunk.root.text)\n",
    "            if ntype == u.NounType.TOOL:\n",
    "                step.tools.append(chunk.text)\n",
    "            else:\n",
    "                # Check if the referenced noun is an ingredient.\n",
    "                ingr_inds = find_ingredient(name, step.state.remaining)\n",
    "                if ingr_inds:\n",
    "                    # Assume ambiguous ingredient means inclusive\n",
    "                    offset = 0\n",
    "                    for i in ingr_inds:\n",
    "                        ingr = r.Ingredient(step.state.remaining[i-offset].name,\n",
    "                                        step.state.remaining[i-offset].quantity,\n",
    "                                        step.state.remaining[i-offset].unit)\n",
    "                        del step.state.remaining[i-offset]\n",
    "                        offset += 1\n",
    "                        step.ingredients.append(ingr)\n",
    "\n",
    "        # Extract times\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'TIME' or ent.label_ == 'DATE':\n",
    "                step.times.append(ent.text)\n",
    "\n",
    "        # Save step to recipe\n",
    "        recipe.steps.append(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class RecipeHTMLParser(HTMLParser):\n",
    "    '''HTML parser that handles recipes'''\n",
    "\n",
    "    def __init__(self, source: u.RecipeSource, convert_charrefs: bool = True) -> None:\n",
    "        # Initialize class, setting recipe to empty\n",
    "        self.source = source\n",
    "        self.recipe = r.Recipe()\n",
    "        self.current_tag = u.HTMLTag.UNKNOWN\n",
    "        self.current_section = u.HTMLTag.UNKNOWN\n",
    "        super().__init__(convert_charrefs=convert_charrefs)\n",
    "    \n",
    "    def handle_starttag(self, tag: str, attrs: list[tuple[str, str | None]]) -> None:\n",
    "        # Save the current tag\n",
    "        self.current_tag = u.HTMLTag.from_tag(self.source, tag, attrs)\n",
    "        match self.current_tag:\n",
    "            case u.HTMLTag.INGREDIENTS_LIST:\n",
    "                self.current_section = u.HTMLTag.INGREDIENTS_LIST\n",
    "            case u.HTMLTag.INGREDIENT:\n",
    "                if self.current_section == u.HTMLTag.INGREDIENTS_LIST:\n",
    "                    self.ingredient = r.Ingredient()\n",
    "            case u.HTMLTag.STEPS_LIST:\n",
    "                self.current_section = u.HTMLTag.STEPS_LIST\n",
    "        return super().handle_starttag(tag, attrs)\n",
    "    \n",
    "    def handle_data(self, data: str) -> None:\n",
    "        # Handle text between tags as appropriate\n",
    "        match self.current_tag:\n",
    "            case u.HTMLTag.TITLE:\n",
    "                self.recipe.title = data.strip()\n",
    "            case u.HTMLTag.OVERVIEW_LABEL:\n",
    "                self.label = data.lower().strip(':,.! \\n\\t')\n",
    "            case u.HTMLTag.OVERVIEW_TEXT:\n",
    "                self.recipe.other[self.label] = data.strip()\n",
    "            case u.HTMLTag.INGREDIENT_QUANTITY:\n",
    "                if self.current_section == u.HTMLTag.INGREDIENTS_LIST:\n",
    "                    self.ingredient.quantity = u.str_to_fraction(data.strip())\n",
    "            case u.HTMLTag.INGREDIENT_UNIT:\n",
    "                if self.current_section == u.HTMLTag.INGREDIENTS_LIST:\n",
    "                    self.ingredient.unit = data.strip()\n",
    "            case u.HTMLTag.INGREDIENT_NAME:\n",
    "                if self.current_section == u.HTMLTag.INGREDIENTS_LIST:\n",
    "                    self.ingredient.name = data.strip()\n",
    "                    self.recipe.ingredients.append(self.ingredient)\n",
    "            case u.HTMLTag.STEP:\n",
    "                if self.current_section == u.HTMLTag.STEPS_LIST:\n",
    "                    parse_and_add_step(data.strip(), self.recipe)\n",
    "        return super().handle_data(data)\n",
    "\n",
    "    def handle_endtag(self, tag: str) -> None:\n",
    "        # Reset tag\n",
    "        self.current_tag = u.HTMLTag.UNKNOWN\n",
    "        return super().handle_endtag(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "def get_recipe_from_url(url: str) -> r.Recipe | None:\n",
    "    '''Retrieves the text of a recipe from a given URL'''\n",
    "\n",
    "    # Find recipe source; return None if unsupported\n",
    "    source = u.RecipeSource.from_url(url)\n",
    "    if source == u.RecipeSource.UNKNOWN:\n",
    "        return None\n",
    "\n",
    "    # Add appropriate HTTPS tag if not there\n",
    "    if not re.match(r'https://www\\.', url):\n",
    "        if re.match(r'www\\.', url):\n",
    "            url = ''.join(['https://', url])\n",
    "        else:\n",
    "            url = ''.join(['https://www.', url])\n",
    "    \n",
    "    # Get the recipe from the page\n",
    "    with requests.get(url) as f:\n",
    "        parser = RecipeHTMLParser(source)\n",
    "        parser.feed(f.text)\n",
    "        return parser.recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moussaka \n",
      "\n",
      "3 eggplants, peeled and cut lengthwise into 1/2 inch thick slices\n",
      "salt to taste\n",
      "1/4 cup olive oil\n",
      "1 tablespoon butter\n",
      "1 pound lean ground beef\n",
      "2 onions, chopped\n",
      "1 clove garlic, minced\n",
      "ground black pepper to taste\n",
      "2 tablespoons dried parsley\n",
      "1/2 teaspoon fines herbs\n",
      "1/4 teaspoon ground cinnamon\n",
      "1/2 teaspoon ground nutmeg, divided\n",
      "1 (8 ounce) can tomato sauce\n",
      "1/2 cup red wine\n",
      "1 egg, beaten\n",
      "4 cups milk\n",
      "1/2 cup butter\n",
      "6 tablespoons all-purpose flour\n",
      "ground white pepper, to taste\n",
      "3/2 cups freshly grated Parmesan cheese\n",
      "\n",
      "Lay eggplant slices on paper towels. ['lay'] ['eggplants, peeled and cut lengthwise into 1/2 inch thick slices'] [] []\n",
      "Sprinkle lightly with salt. ['sprinkle'] ['salt to taste'] [] []\n",
      "Let sit for 30 minutes to draw out moisture. ['sit'] [] [] ['30 minutes']\n",
      "Pat dry with paper towels. ['dry'] [] [] []\n",
      "Warm olive oil in a skillet over high heat. ['warm'] ['olive oil'] ['a skillet'] []\n",
      "Fry eggplant until browned, 2 to 3 minutes per side. ['fry'] [] [] ['2 to 3 minutes']\n",
      "Drain on paper towels. ['drain'] [] [] []\n",
      "Set aside. ['set'] [] [] []\n",
      "Melt 1 tablespoon butter in a large skillet over medium heat. ['melt'] [] ['a large skillet'] []\n",
      "Stir in ground beef, onions, and garlic. ['stir'] ['lean ground beef', 'onions, chopped', 'garlic, minced'] [] []\n",
      "Season with salt and black pepper. ['season'] ['ground black pepper to taste'] [] []\n",
      "Cook and stir until beef is browned, 8 to 10 minutes. ['cook', 'stir'] [] [] ['8 to 10 minutes']\n",
      "Add parsley, fines herbs, cinnamon, and 1/4 teaspoon nutmeg. ['add'] ['dried parsley', 'fines herbs', 'ground cinnamon'] [] []\n",
      "Pour in tomato sauce and wine. ['pour'] ['tomato sauce', 'red wine'] [] []\n",
      "Mix well. ['mix'] [] [] []\n",
      "Simmer for 20 minutes. ['simmer'] [] [] ['20 minutes']\n",
      "Allow to cool. ['allow'] [] [] []\n",
      "Stir in beaten egg. ['stir'] ['egg, beaten'] [] []\n",
      "Scald milk in a saucepan over medium heat. ['scald'] [] ['a saucepan'] []\n",
      "At the same time, melt 1/2 cup butter in a large skillet over medium heat. ['melt'] [] ['a large skillet'] []\n",
      "Whisk flour into butter until smooth. ['whisk'] ['butter'] [] []\n",
      "Lower heat. ['lower'] [] [] []\n",
      "Gradually pour in scalded milk, whisking constantly until it thickens. ['pour'] ['milk'] [] []\n",
      "Season béchamel sauce with salt and white pepper. ['season'] ['ground white pepper, to taste'] [] ['season']\n",
      "Preheat the oven to 350 degrees F (175 degrees C). ['preheat'] [] ['the oven'] []\n",
      "Grease a 9x13-inch baking dish. ['grease'] [] ['a 9x13-inch baking dish'] []\n",
      "Arrange a single layer of eggplant in the prepared baking dish. ['arrange'] [] ['the prepared baking dish'] []\n",
      "Cover eggplant with meat sauce. ['cover'] [] [] []\n",
      "Sprinkle 1/2 cup Parmesan cheese on top. ['sprinkle'] ['freshly grated Parmesan cheese'] ['1/2 cup'] []\n",
      "Cover with remaining eggplant and sprinkle another 1/2 cup cheese on top. ['cover', 'sprinkle'] [] [] []\n",
      "Pour béchamel sauce on top and sprinkle with remaining 1/4 teaspoon nutmeg. ['pour', 'sprinkle'] [] [] []\n",
      "Cover with remaining cheese. ['cover'] [] [] []\n",
      "Bake in the preheated oven until bubbly and browned, about 1 hour. ['bake', 'browned'] [] ['the preheated oven'] ['about 1 hour']\n",
      "Serve hot and enjoy!. ['serve'] [] [] []\n"
     ]
    }
   ],
   "source": [
    "recipe = get_recipe_from_url(\"https://www.allrecipes.com/recipe/19644/moussaka/\")\n",
    "# recipe = get_recipe_from_url(\"https://www.allrecipes.com/recipe/218091/classic-and-simple-meat-lasagna/\")\n",
    "# recipe = get_recipe_from_url(\"https://www.allrecipes.com/recipe/230238/gingerbread-men-cookies/\")\n",
    "print(recipe.title, '\\n')\n",
    "for ingr in recipe.ingredients:\n",
    "    print(ingr.quantity if ingr.quantity else '', ' ' if ingr.quantity else '',\n",
    "          ingr.unit if ingr.unit else '', ' ' if ingr.unit else '',\n",
    "          ingr.name, sep='')\n",
    "print()\n",
    "for step in recipe.steps:\n",
    "    print(step.text, step.methods, [ingr.name for ingr in step.ingredients],\n",
    "          step.tools, step.times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lay VERB ROOT lay VERB [slices, on] \n",
      "eggplant NOUN amod slices NOUN [] \n",
      "slices NOUN dobj lay VERB [eggplant] \n",
      "on ADP prep lay VERB [towels] \n",
      "paper NOUN compound towels NOUN [] \n",
      "towels NOUN pobj on ADP [paper] \n",
      "\n",
      "eggplant slices slices dobj lay 1 3\n",
      "paper towels towels pobj on 4 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "text = \"lay eggplant slices on paper towels\"\n",
    "# text = \"sprinkle lightly with salt.\"\n",
    "# text = \"cook and stir until beef is browned, 8 to 10 minutes\"\n",
    "# text = \"stir in ground beef, onions, and garlic\"\n",
    "# text = \"meanwhile, place ground beef, garlic, oregano, garlic powder, salt, and black pepper in a large skillet over medium heat\"\n",
    "# text = \"add lasagna noodles and cook for 10 minutes or until al dente\"\n",
    "# text = \"preheat the oven to 350 degrees F (175 degrees C)\"\n",
    "# text = \"season béchamel sauce with salt and white pepper\"\n",
    "# text = \"serve hot and enjoy!\"\n",
    "# text = \"bake in the preheated oven until edges of cookies are set and just begin to brown, 8 to 10 minutes\"\n",
    "# text = \"pour béchamel sauce on top and sprinkle with remaining 1/4 teaspoon nutmeg\"\n",
    "# text = \"add parsley, fines herbs, cinnamon, and 1/4 teaspoon nutmeg\"\n",
    "\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children], token.ent_type_)\n",
    "print()\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text, chunk.start, chunk.end)\n",
    "print()\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('paper.n.01'), Synset('composition.n.08'), Synset('newspaper.n.01'), Synset('paper.n.04'), Synset('paper.n.05'), Synset('newspaper.n.02'), Synset('newspaper.n.03')]\n",
      "[[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('matter.n.03'), Synset('substance.n.01'), Synset('material.n.01'), Synset('paper.n.01')], [Synset('entity.n.01'), Synset('abstraction.n.06'), Synset('relation.n.01'), Synset('part.n.01'), Synset('substance.n.01'), Synset('material.n.01'), Synset('paper.n.01')]]\n",
      "[[Synset('entity.n.01'), Synset('abstraction.n.06'), Synset('communication.n.02'), Synset('written_communication.n.01'), Synset('writing.n.02'), Synset('essay.n.01'), Synset('composition.n.08')]]\n",
      "[[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('artifact.n.01'), Synset('instrumentality.n.03'), Synset('medium.n.01'), Synset('print_media.n.01'), Synset('press.n.02'), Synset('newspaper.n.01')]]\n",
      "[[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('artifact.n.01'), Synset('instrumentality.n.03'), Synset('medium.n.01'), Synset('paper.n.04')]]\n",
      "[[Synset('entity.n.01'), Synset('abstraction.n.06'), Synset('communication.n.02'), Synset('expressive_style.n.01'), Synset('writing_style.n.01'), Synset('prose.n.01'), Synset('nonfiction.n.01'), Synset('article.n.01'), Synset('paper.n.05')], [Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('artifact.n.01'), Synset('creation.n.02'), Synset('piece.n.06'), Synset('article.n.01'), Synset('paper.n.05')]]\n",
      "[[Synset('entity.n.01'), Synset('abstraction.n.06'), Synset('group.n.01'), Synset('social_group.n.01'), Synset('organization.n.01'), Synset('enterprise.n.02'), Synset('business.n.01'), Synset('firm.n.01'), Synset('publisher.n.01'), Synset('newspaper.n.02')]]\n",
      "[[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('artifact.n.01'), Synset('creation.n.02'), Synset('product.n.02'), Synset('newspaper.n.03')]]\n",
      "\n",
      "NounType.UNKNOWN\n",
      "NounType.TOOL\n",
      "NounType.TOOL\n",
      "NounType.UNKNOWN\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def print_hypernym_paths(noun: str):\n",
    "    mix_sets = wn.synsets(noun, wn.NOUN)\n",
    "    print(mix_sets)\n",
    "    for s in mix_sets:\n",
    "        print(s.hypernym_paths())\n",
    "    print()\n",
    "\n",
    "print_hypernym_paths(\"paper\")\n",
    "print(u.NounType.from_str(\"season\"))\n",
    "print(u.NounType.from_str(\"tin\"))\n",
    "print(u.NounType.from_str(\"mixer\"))\n",
    "print(u.NounType.from_str(\"men\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
